# API-Based ETL Pipeline (Python)

## ğŸ“Œ Project Overview
This project implements an end-to-end ETL (Extract, Transform, Load) pipeline using Python.
The pipeline fetches data from external REST APIs, transforms raw JSON into analytics-ready
tables, and loads the processed data into a relational database for reporting and analysis.

This project simulates a real-world data engineering workflow commonly used in analytics
and business intelligence environments.

## ğŸ›  Tech Stack
- Python
- Pandas
- Requests
- SQLite
- SQL
- Git & GitHub

## ğŸ”„ ETL Workflow

### 1ï¸âƒ£ Extract
- Data is retrieved from public REST APIs
- Endpoints used:
  - Users API
  - Posts API
- Retry logic and timeouts implemented for reliability

### 2ï¸âƒ£ Transform
- JSON data normalized into tabular format
- Data cleaning and validation applied
- Dimension and fact tables created
- Aggregated KPIs generated

### 3ï¸âƒ£ Load
- Transformed data loaded into SQLite database
- Tables created:
  - `dim_users`
  - `fact_posts`
  - `user_post_summary`
    
## ğŸ›  Tools & Technologies (Conceptual)
- **Python** â†’ ETL logic implementation
- **REST APIs** â†’ Data source
- **Pandas** â†’ Data transformation
- **SQLite** â†’ Relational storage
- **GitHub** â†’ Version control and collaboration

## ğŸ¯ Key Learning Outcomes
- Understanding end-to-end ETL workflows
- Designing scalable ETL architectures
- Working with API-based data sources
- Preparing data for analytics and reporting
- Applying data engineering best practices
